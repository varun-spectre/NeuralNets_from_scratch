{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 10:48:48.332733 140151043381056 deprecation.py:323] From <ipython-input-2-71bdf6b57526>:3: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 10:48:48.585269 140151043381056 deprecation.py:323] From <ipython-input-2-71bdf6b57526>:5: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import extract_images, extract_labels\n",
    "with open('./train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_images = extract_images(f)\n",
    "with open('./train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_labels = extract_labels(f)\n",
    "with open('./t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_images = extract_images(f)\n",
    "with open('./t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_labels = extract_labels(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizes of our test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n",
      "(10000, 28, 28, 1)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of testing examples: 10000\n",
      "Each image is of size: (28, 28, 1)\n",
      "train_images shape: (60000, 28, 28, 1)\n",
      "train_labels shape: (60000,)\n",
      "test_images shape: (10000, 28, 28, 1)\n",
      "test_labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "m_train = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "m_test = test_images.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 1)\")\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening the image and reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_flatten = train_images.reshape(train_images.shape[0],-1).T\n",
    "test_images_flatten = test_images.reshape(test_images.shape[0],-1).T\n",
    "train_x = train_images_flatten/255\n",
    "test_x = test_images_flatten/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(784, 10000)\n",
      "***************\n",
      "(784, 60000)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(train_images_flatten.shape)\n",
    "print(test_images_flatten.shape)\n",
    "print(\"***************\")\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## converting label data inti one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = 10\n",
    "examples = train_labels.shape[0]\n",
    "train_labels = train_labels.reshape(1, examples)\n",
    "Y_new = np.eye(digits)[train_labels.astype('int32')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "\n",
    "examples = test_labels.shape[0]\n",
    "test_labels = test_labels.reshape(1,examples)\n",
    "Y_new_test = np.eye(digits)[test_labels.astype('int32')]\n",
    "Y_new_test = Y_new_test.T.reshape(digits, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_x\n",
    "X_test = test_x\n",
    "Y_train = Y_new\n",
    "Y_test = Y_new_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test data size after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(784, 10000)\n",
      "(10, 60000)\n",
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sigmoid function\n",
    "\"\"\"\n",
    "def sigmoid(z):\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    return s\n",
    "\"\"\"\n",
    "loss function\n",
    "\"\"\"\n",
    "def compute_loss(Y, Y_hat):\n",
    "\n",
    "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    m = Y.shape[1]\n",
    "    L = -(1./m) * L_sum\n",
    "\n",
    "    return L\n",
    "\"\"\"\n",
    "Forward prop\n",
    "\"\"\"\n",
    "def feed_forward(X, params):\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "    cache[\"A2\"] = np.exp(cache[\"Z2\"]) / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "\n",
    "    return cache\n",
    "\"\"\"\n",
    "Backward prop\n",
    "\"\"\"\n",
    "def back_propagate(X, Y, params, cache):\n",
    "\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "    dW2 = (1./m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1./m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "    dW1 = (1./m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training cost = 0.15247891859267396, test cost = 0.1547560029527509\n",
      "Epoch 2: training cost = 0.10002037166708286, test cost = 0.1143751924400732\n",
      "Epoch 3: training cost = 0.07345533436456224, test cost = 0.09667640947427097\n",
      "Epoch 4: training cost = 0.06631645063042363, test cost = 0.09893790330148221\n",
      "Epoch 5: training cost = 0.05696481716210169, test cost = 0.09308700623494924\n",
      "Epoch 6: training cost = 0.043058871259209014, test cost = 0.08732451509837161\n",
      "Epoch 7: training cost = 0.04041933333490396, test cost = 0.08644605473893195\n",
      "Epoch 8: training cost = 0.03311140650773119, test cost = 0.0813292627297561\n",
      "Epoch 9: training cost = 0.02904318969196912, test cost = 0.08464942196754249\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(138)\n",
    "m = 60000\n",
    "# hyperparameters\n",
    "n_x = X_train.shape[0]\n",
    "n_h = 64\n",
    "\"\"\"\n",
    "Defining learning rate to be 4 as we are using momentum instead of gradient descent. \n",
    "with learning rate as 4 we are able to achieve 97% accuracy in 9 iterations\n",
    "\"\"\"\n",
    "learning_rate = 4\n",
    "beta = .9\n",
    "\"\"\"\n",
    "Defining batch size to be 128\n",
    "Advantages of using Mini batch gradient descent\n",
    "--> you have the vectorization advantage\n",
    "--> make progress without waiting to process the entire training set\n",
    "Disadvantages\n",
    "--> Doesn't always exactly converge (oscillates in a very small region, but you can reduce learning rate)\n",
    "\"\"\"\n",
    "batch_size = 128\n",
    "batches = -(-m // batch_size)\n",
    "\n",
    "# initialization\n",
    "params = { \"W1\": np.random.randn(n_h, n_x) * np.sqrt(1. / n_x),\n",
    "           \"b1\": np.zeros((n_h, 1)) * np.sqrt(1. / n_x),\n",
    "           \"W2\": np.random.randn(digits, n_h) * np.sqrt(1. / n_h),\n",
    "           \"b2\": np.zeros((digits, 1)) * np.sqrt(1. / n_h) }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Intializing params for Gradient descent with momentum\n",
    "\n",
    "\"\"\"\n",
    "V_dW1 = np.zeros(params[\"W1\"].shape)\n",
    "V_db1 = np.zeros(params[\"b1\"].shape)\n",
    "V_dW2 = np.zeros(params[\"W2\"].shape)\n",
    "V_db2 = np.zeros(params[\"b2\"].shape)\n",
    "\n",
    "# train\n",
    "for i in range(9):\n",
    "\n",
    "    permutation = np.random.permutation(X_train.shape[1])\n",
    "    X_train_shuffled = X_train[:, permutation]\n",
    "    Y_train_shuffled = Y_train[:, permutation]\n",
    "\n",
    "    for j in range(batches):\n",
    "\n",
    "        begin = j * batch_size\n",
    "        end = min(begin + batch_size, X_train.shape[1] - 1)\n",
    "        X = X_train_shuffled[:, begin:end]\n",
    "        Y = Y_train_shuffled[:, begin:end]\n",
    "        m_batch = end - begin\n",
    "\n",
    "        cache = feed_forward(X, params)\n",
    "        grads = back_propagate(X, Y, params, cache)\n",
    "\n",
    "        V_dW1 = (beta * V_dW1 + (1. - beta) * grads[\"dW1\"])\n",
    "        V_db1 = (beta * V_db1 + (1. - beta) * grads[\"db1\"])\n",
    "        V_dW2 = (beta * V_dW2 + (1. - beta) * grads[\"dW2\"])\n",
    "        V_db2 = (beta * V_db2 + (1. - beta) * grads[\"db2\"])\n",
    "\n",
    "        params[\"W1\"] = params[\"W1\"] - learning_rate * V_dW1\n",
    "        params[\"b1\"] = params[\"b1\"] - learning_rate * V_db1\n",
    "        params[\"W2\"] = params[\"W2\"] - learning_rate * V_dW2\n",
    "        params[\"b2\"] = params[\"b2\"] - learning_rate * V_db2\n",
    "\n",
    "    cache = feed_forward(X_train, params)\n",
    "    train_cost = compute_loss(Y_train, cache[\"A2\"])\n",
    "    cache = feed_forward(X_test, params)\n",
    "    test_cost = compute_loss(Y_test, cache[\"A2\"])\n",
    "    print(\"Epoch {}: training cost = {}, test cost = {}\".format(i+1 ,train_cost, test_cost))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       992\n",
      "           1       0.99      0.99      0.99      1132\n",
      "           2       0.99      0.95      0.97      1069\n",
      "           3       0.98      0.97      0.97      1020\n",
      "           4       0.97      0.98      0.98       964\n",
      "           5       0.97      0.99      0.98       874\n",
      "           6       0.97      0.96      0.97       967\n",
      "           7       0.96      0.98      0.97      1014\n",
      "           8       0.96      0.98      0.97       952\n",
      "           9       0.97      0.96      0.97      1016\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "0.9746\n"
     ]
    }
   ],
   "source": [
    "cache = feed_forward(X_test, params)\n",
    "predictions = np.argmax(cache[\"A2\"], axis=0)\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "\n",
    "print(classification_report(predictions, labels))\n",
    "print(accuracy_score(predictions,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
